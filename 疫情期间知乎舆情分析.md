# 第2组 疫情期间知乎舆情分析

# 项目信息
### 组号
第2组-疫情期间知乎舆情监控
### 小组成员
| **姓名** | **学号** |
| --- | --- |
| 张卓楠 | 181830249 |
| 刘学卓 | 181250088 |
| 王嘉伟 | 181250135 |
| 邱星曜 | 181830154 |
| 李唐婧 | 181850074 |

# 1 应用场景
本小组关注新冠肺炎疫情期间知乎相关问答舆情的发展趋势，提出了以下两个待解决问题：

- 疫情期间每天的热点话题识别
   - 通过热点话题我们可以准确检测/回溯每一天的舆情发展趋势
- 疫情期间知乎公众平台上问答的情感倾向分析
   - 通过分析知乎上每天回答的情感倾向，我们可以了解公众对于事态发展变化的态度，有助于我们以此为根据做出决策判断
# 2 数据来源&数据内容
## 2.1 数据来源
本项目数据来源于[知乎：新型冠状病毒话题](https://www.zhihu.com/topic/21238418/top-answers)
## 2.2 数据获取方式
使用scrapy框架进行爬虫。分为三个爬虫，分别爬取知乎话题精华部分下的所有问题、所有问题的所有答案、每天更新的答案。


**spiders部分**
爬取问题


```python
'''
 爬取问题并解析
'''

def parse(self, response):
    # 保存为QuestionItem()交给piplines处理
    Item = QuestionItem()
    data = json.loads(response.body)
    dataList = data['data']
    nextUrl = data['paging']
    # 判断是否还有下一个ajax请求
    if nextUrl['is_end'] or len(data['data']) == 0:
        return
    else:
        # 加入新的url
        self.question_next_url_list.append(nextUrl['next'])
    for item in dataList:
        dic = dict(item['target'])
        # 区分是专栏还是问题
        if 'question' in dic:
            # 保存问题id以及标题
            question_id = dic["question"]['id']
            title = dic['question']['title']
            Item['id'] = question_id
            Item['title'] = title
            yield Item
    self.question_index += 1
    if not (nextUrl['is_end'] or len(data['data']) == 0):
        yield Request(self.question_next_url_list[self.question_index], callback=self.parse)
```


爬取答案
```python
'''
从mongodb中读取所有问题的id，爬取这些问题中的所有答案
'''

def __init__(self,*args, **kwargs):
    self.host = MONGODB_HOST
    self.port = MONGODB_PORT
    self.database = MONGODB_DBNAME
    self.sheet = MONGODB_Q_SHEET_NAME
    client = pymongo.MongoClient(host=self.host, port=self.port)
    db = client[self.database]
    collection = db[self.sheet]
    # 从mongodb中读取所有问题的id
    for item in collection.find():
        print(item['id'])
        print(item['title'])
        self.start_urls.append(self.base_url.format(item['id']))
           
            
def parse(self, response):
    # 保存为AnswerItem交给piplines处理
    Item = AnswerItem()
    data = json.loads(response.body)
    dataList = data['data']
    nextUrl = data['paging']
    if not nextUrl['is_end']:
        self.start_urls.append(nextUrl['next'])
    for d in dataList:
        content = d['content']
        # 收集到的content带有前端标签，利用lxml的etree去除标签
        response = etree.HTML(text=content)
        content = response.xpath('string(.)')
        Item['content'] = content
        Item['title'] = d['question']['title']
        Item['created_time'] = timeTransfer(d['created_time'])
        yield Item
        print(d['question']['title'])
        print("-------------------------")
        print(content)
        print(timeTransfer(d['created_time']))
    pass
```
## 2.3 数据格式
一共有三种数据问题数据、答案数据、答案切词数据。
#### 问题数据
```json
{
	"question_id": 问题id,
	"title": 问题标题
}
```
#### 答案数据
```json
{
    "title" : 问题标题,
    "content": 答案内容,
    "created_time" = 答案发布时间
}
```
#### 答案切词数据
```json
{
	"month":切词结果
}
```
## 2.4 存储方式
数据爬取后以Item的形式，由piplines存入mongodb


```python
# 首先引入settings.py中定义的mongodb端口、数据库名、表名等数据
# 连接mongodb
def __init__(self):
        host = MONGODB_HOST
        port = MONGODB_PORT
        dbname = MONGODB_DBNAME
        client = pymongo.MongoClient(host=host, port=port)
        self.my_db = client[dbname]
	
    def process_item(self, item, spider):
        # 如果是question，交给process_question处理，如果是answer，交给process_answer处理
        if isinstance(item, QuestionItem):
            self.process_question(item)
        else:
            self.process_answer(item)
            
    def process_question(self, item):
        sheet_name = MONGODB_Q_SHEET_NAME
        self.sheet = self.my_db[sheet_name]
        data = dict(item)
        self.sheet.insert(data)
        return item

    def process_answer(self, item):
        sheet_name = MONGODB_A_SHEET_NAME
        self.sheet = self.my_db[sheet_name]
        data = dict(item)
        self.sheet.insert(data)
        return item
```
## 2.5 预处理过程
### 2.5.1 格式处理/数据清洗

- 在爬取过程中，使用lxml中的etree对带网页标签的文本进行处理
- 在分词过程中，参考百度停用词表，对停用词进行剔除
### 2.5.2 文本切词
利用jieba全模式对爬取到的文本进行分词处理，同时剔除停用词。


```python
def cal(string):
    # 停用词集合
    stop_words = readFile('baidu_stopwords.txt')
    # 分词，同时剔除停用词
    word_list = [word for word in jieba.cut(string, cut_all=False) if word not in stop_words]
    dic_words = dict(Counter(word_list))
    return dic_words
```
### 2.5.3 情感分析
使用BERT官方的开源代码进行中文文本情感分类任务，借助google提供的预训练模型，使用标定好的情感分类数据集进行fine tune，并对我们的知乎回答数据进行情感标签分类。
#### 2.5.3.1 代码结构

- 预训练脚本：train.sh
- 预测脚本：predict.sh
- 单条语句预测：intent.py
- 预训练模型：[chinese_L-12_H-768_A-12](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)
   - bert_model.ckpt：负责模型变量载入的

   - vocab.txt：训练时中文文本采用的字典

   - bert_config.json：BERT在训练时，可选调整的一些参数

- 语料：
   - 携程酒店评论语料-三分类：data/threeClassifier

   - 中文情感分析语料chnsenticorp-二分类：data/chnsenticor

   - 知乎语料-未标定：data/zhihu

#### 2.5.3.2 具体实现
**预训练模型**
对于中文而言，google公布了一个参数较小的BERT预训练模型。[https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)
#### 2.5.3.3 fine tune
BERT的代码同论文里描述的一致，主要分为两个部分。一个是训练语言模型（language model）的预训练（pretrain）部分。另一个是训练具体任务(task)的fine-tune部分。在开源的代码中：

- 预训练入口：run_pretraining.py

- fine-tune入口：针对不同的任务分别在run_classifier.py和run_squad.py。
   - run_classifier.py：分类任务。例如CoLA、MRPC、MultiNLI。

   - run_squad.py：阅读理解(MRC)任务，如squad2.0和squad1.1。


因此如果要在自己的数据集上fine-tune跑分类任务代码，需要编写类似run_classifier.py的具体任务文件。
#### 2.5.3.4 代码部分
代码部分的实现，主要涉及以下部分：

- run_classifier.py：class MyProcessor
   - get_train_examples：读取训练语料，返回list[InputExample]，进行训练任务

   - get_dev_examples：读取评估语料，返回list[InputExample]，进行评估任务

   - get_test_examples：读取预测数据，返回list[InputExample]，进行预测任务

   - get_labels：设置标签，根据语料分类的不同设置不同的标签

   - 对于一个需要执行训练、交叉验证和测试完整过程的模型而言，自定义的processor里需要继承DataProcessor，并重载获取label的get_labels和获取单个输入的get_train_examples,get_dev_examples和get_test_examples函数。其分别会在main函数的FLAGS.do_train、FLAGS.do_eval和FLAGS.do_predict阶段被调用。

- run_classifier.py：main
   - processors：添加自己的MyProcessor

   - 设置训练、评估、预测任务的输入输出格式及文件夹即可

- single_predict.py&intent.py
   - 类似于run_classifier.py的main方法，实现了predicts方法以实现语句级别的预测

   - intent.py为一个执行demo

**MyProcessor**
```python
class MyProcessor(DataProcessor):

    # read txt
    # 返回InputExample类组成的list
    # text_a是一串字符串，text_b则是另一串字符串。在进行后续输入处理后(BERT代码中已包含，不需要自己完成)
    # text_a和text_b将组合成[CLS] text_a [SEP] text_b [SEP]的形式传入模型
    def get_train_examples(self, data_dir):
        file_path = os.path.join(data_dir, 'train_sentiment.txt')
        f = open(file_path, 'r', encoding='utf-8')
        train_data = []
        index = 0
        for line in f.readlines():
            guid = 'train-%d' % index  # 参数guid是用来区分每个example的
            line = line.replace("\n", "").split("\t")
            text_a = tokenization.convert_to_unicode(str(line[1]))  # 要分类的文本
            label = str(line[2])  # 文本对应的情感类别
            train_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))  
            # 加入到InputExample列表中
            index += 1
        return train_data

    # read txt
    # 返回InputExample类组成的list
    # text_a是一串字符串，text_b则是另一串字符串。在进行后续输入处理后(BERT代码中已包含，不需要自己完成)
    # text_a和text_b将组合成[CLS] text_a [SEP] text_b [SEP]的形式传入模型
    def get_dev_examples(self, data_dir):
        file_path = os.path.join(data_dir, 'test_sentiment.txt')
        f = open(file_path, 'r', encoding='utf-8')
        dev_data = []
        index = 0
        for line in f.readlines():
            guid = 'dev-%d' % index # 参数guid是用来区分每个example的
            line = line.replace("\n", "").split("\t")
            text_a = tokenization.convert_to_unicode(str(line[1])) # 要评估的文本
            label = str(line[2]) # 文本对应的情感类别
            dev_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
            # 加入到InputExample列表中
            index += 1
        return dev_data

    # csv
    # 读取zhihu回答的csv文件进行相关
    def get_test_examples(self, data_dir):
        file_path = os.path.join(data_dir, 'answer.csv')
        test_df = pd.read_csv(file_path, encoding='utf-8', header=None, sep='\n')
        test_data = []
        for index, test in enumerate(test_df.values):
            guid = 'test-%d' % index
            text_a = tokenization.convert_to_unicode(str(test[0]).split("\t")[1])
            # 传入一个随意的label数值，因为在模型的预测（prediction）中label将不会参与计算！！
            test_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=str(0)))
        return test_data

    def get_labels(self):
        # 分别对应三类情感标签
        return ['0', '1', '2']
```
修改完成MyProcessor后，需要在在原本main函数的processor字典里，加入修改后的processor类，即可在运行参数里指定调用该processor。
```python
    processors = {
        # "cola": ColaProcessor,
        # "mnli": MnliProcessor,
        # "mrpc": MrpcProcessor,
        # "xnli": XnliProcessor,
        "my": MyProcessor,
    }
```
**single_predict.py**
```python
def predicts(text_data):
    ...
    
    # predict_examples = processor.get_test_examples(FLAGS.data_dir) 
    # 不使用get_test_examples批量处理，而是直接读取list
    test_data = []
    for index in range(len(text_data)):
        guid = 'test-%d' % index
        text_a = tokenization.convert_to_unicode(str(text_data[index]))
        # text_b = tokenization.convert_to_unicode(str(test[1]))
        # label = str(test[2])
        test_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=None))
        
	...
```
#### 2.5.3.5训练&测试
使用train.sh进行fine tune训练，并进行评估：
```shell
#!/usr/bin/env bash

python run_classifier.py \
  --data_dir=data \
  --task_name=my \
  --vocab_file=chinese_L-12_H-768_A-12/vocab.txt \
  --bert_config_file=chinese_L-12_H-768_A-12/bert_config.json \
  --output_dir=my_model \
  --do_train=true \
  --do_eval=true \
  --init_checkpoint=chinese_L-12_H-768_A-12/bert_model.ckpt \
  --max_seq_length=70 \
  --train_batch_size=32 \
  --learning_rate=5e-5 \
  --num_train_epochs=3.0
```
训练结果：
```
eval_accuracy = 0.90064484
eval_loss = 0.30996767
global_step = 1605
loss = 0.30984673
```
使用predict.sh执行批量预测任务：
```shell
#!/usr/bin/env bash
python run_classifier.py \
  --task_name=my \
  --do_predict=true \
  --data_dir=data \
  --vocab_file=chinese_L-12_H-768_A-12/vocab.txt \
  --bert_config_file=chinese_L-12_H-768_A-12/bert_config.json \
  --init_checkpoint=my_model \
  --max_seq_length=70 \
  --output_dir=output
```
单句预测执行：
```python
from single_predict import predicts
sentences = ['发热，呼吸道症状为主，潜伏期现在观察来看14天为主，注意14天内去过武汉或者与去过武汉的不明原因发热或者不明原因肺炎的人员接触过的人出现发热，或者不明原因肺炎的。',
             '这是一种简单、实用的型号，国内可能便宜一些']
dic = predicts(sentences)
for item in dic.items():
    print(item)
```
# 3 流计算总体架构
## 3.1 流准备和监听
### 3.1.1 流架构
将上一步爬虫爬下来的数据+利用kafka生产者传入kafka消息队列，利用sparkstreaming中监听kafka消息的接口进行监听，将监听到的streaming(dataframe格式)利用spark中的mapreduce进行疫情期间每日的词频统计以及每日文本情感分析结果统计，将统计结果转化为json格式输出，再利用前端进行展示。
### 3.1.2 流配置
采用spark2.4.7中的pyspark模块、kafka2.16以及对应spark-kafka-assembly jar包进行kafka与spark间的接口配置。（配置过程较为艰辛，经常有报错）
## 3.2 流计算
### 3.2.1 kafka producer模块
![kafka1.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606631890226-6b2515a3-6baa-4cc0-92a9-1bfdcdd4f9af.png#align=left&display=inline&height=679&margin=%5Bobject%20Object%5D&name=kafka1.png&originHeight=679&originWidth=894&size=88287&status=done&style=none&width=894)
这里将对应的分词结果传入kafka消息队列（以byte形式传入，其他格式会报错），该代码仅为展示，实际需要改变其中参数（实际传入过程中），将每一天的数据分别传入，等待sparkstreaming监听
![kafka2.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632040292-89878230-030c-43de-86bf-b56eba01967d.png#align=left&display=inline&height=798&margin=%5Bobject%20Object%5D&name=kafka2.png&originHeight=798&originWidth=970&size=92819&status=done&style=none&width=970)
这里是将文本情感分析结果传入kafka，实际使用需同上文中，改变对应传入天数（以pos位置分割），等待kafka使用
### 3.2.2 sparkstreaming（流分析处理过程）
### ![streaming1.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632293474-d6ee5992-70a7-45f5-a557-3a975748bd5d.png#align=left&display=inline&height=683&margin=%5Bobject%20Object%5D&name=streaming1.png&originHeight=683&originWidth=1307&size=47371&status=done&style=none&width=1307)
以上是sparkstreaming监听kafka消息队列中分析结果的代码。具体分析如下：
![streaming_1.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632489378-20579d9b-7f00-4fe4-a96f-c811d7428472.png#align=left&display=inline&height=47&margin=%5Bobject%20Object%5D&name=streaming_1.png&originHeight=47&originWidth=752&size=4526&status=done&style=none&width=752)
以上创建sparkconf，注意要配置set，即第二行代码的后半部分，不然回提示报错。
![stream——2.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632594184-0e95c770-50e1-4f41-8d77-5c8676fe8cad.png#align=left&display=inline&height=102&margin=%5Bobject%20Object%5D&name=stream%E2%80%94%E2%80%942.png&originHeight=102&originWidth=1023&size=10879&status=done&style=none&width=1023)  
以上创建sparkstreaming流监听kafka对应端口，利用jar包中的kafkaUtils.createDirectStream来创建监听
![streaming_3.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632729490-f700f2e2-9c7d-4fef-8176-2065dfdcc456.png#align=left&display=inline&height=123&margin=%5Bobject%20Object%5D&name=streaming_3.png&originHeight=123&originWidth=1242&size=11007&status=done&style=none&width=1242)
以上是利用spark rdd的mapreduce进行词频统计，将词频统计结果用saveAsTextFiles保存入本地文件并且继续监听。
![streaming_4.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632863658-3ef0ff09-fadc-43ba-a5b8-aa58125267fa.png#align=left&display=inline&height=536&margin=%5Bobject%20Object%5D&name=streaming_4.png&originHeight=536&originWidth=1297&size=43874&status=done&style=none&width=1297)
以上是监听文本分析结果数据代码
### 3.2.3 结果处理
![json1.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606632988425-9c922b03-6587-41d2-a4f1-c2b6f742f7e2.png#align=left&display=inline&height=644&margin=%5Bobject%20Object%5D&name=json1.png&originHeight=644&originWidth=1061&size=77273&status=done&style=none&width=1061)
因为上一步中，将对应的wordcount结果传回了本地文件，而对应文件并没有日期等信息，所以写了两个函数将其转化为json格式，方便前端调用。
### ![json2.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606633097943-e7aaf632-bd9e-4ce1-8abc-8a8aae94a6c2.png#align=left&display=inline&height=800&margin=%5Bobject%20Object%5D&name=json2.png&originHeight=800&originWidth=1049&size=95639&status=done&style=none&width=1049)
### 3.2.4 代码使用说明
在配置了pyspark的服务器上用python启动代码包中的两个kafkaconsumer（不同console）,然后在本机或者服务器上启动producer向kafka中传递消息，最终可以在console中看到结果，并将结果保存在对应配置文件夹或者hdfs上，最后使用json_convert将其转化为json输出。
## 3.3 计算结果动态展示
### 3.3.1 Spark结算结果保存
sparkstreaming pprint结果样例（2020-01-20情绪分析结果）：
![2020-01-20结果.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606651061555-1868e3c4-cf18-4df0-a798-1781e62a76b2.png#align=left&display=inline&height=315&margin=%5Bobject%20Object%5D&name=2020-01-20%E7%BB%93%E6%9E%9C.png&originHeight=315&originWidth=371&size=7940&status=done&style=none&width=371)
结果保存服务器样例：
![结果保存.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606651237797-5e191cc3-d3f9-4321-8b70-35ef616ef84b.png#align=left&display=inline&height=231&margin=%5Bobject%20Object%5D&name=%E7%BB%93%E6%9E%9C%E4%BF%9D%E5%AD%98.png&originHeight=231&originWidth=591&size=21764&status=done&style=none&width=591)
![结果2.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606651272710-803c1540-f943-4436-b9a5-f8423fa45871.png#align=left&display=inline&height=425&margin=%5Bobject%20Object%5D&name=%E7%BB%93%E6%9E%9C2.png&originHeight=425&originWidth=675&size=9203&status=done&style=none&width=675)
然后运行转换代码：将其转换为json格式：
![结构2.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606651338608-f8519069-1d5b-43dc-a822-09139a040239.png#align=left&display=inline&height=132&margin=%5Bobject%20Object%5D&name=%E7%BB%93%E6%9E%842.png&originHeight=132&originWidth=276&size=5924&status=done&style=none&width=276)
结果以json格式保存。
关于词频使用同上，效果如图：
![json数据样例.png](https://cdn.nlark.com/yuque/0/2020/png/303663/1606650601645-7bb4c783-37b0-4d13-bb9b-ffda2858d4d4.png#align=left&display=inline&height=915&margin=%5Bobject%20Object%5D&name=json%E6%95%B0%E6%8D%AE%E6%A0%B7%E4%BE%8B.png&originHeight=915&originWidth=292&size=37537&status=done&style=none&width=292)
最后，在每次结束后都会将json存入mongodb，代码如下：
```python
import pymongo
from pymongo import MongoClient
import pandas as pd
import time
from pandas import DataFrame

class MongoDbHandler(object):
    def __init__(self,URL):
        print(URL)
        self.__Client = MongoClient(URL)

    def find_all(self, db, collection, condition = None):
        self.__db = self.__Client[db]
        self.__collection = self.__db[collection]
        cursor = self.__collection.find(condition)
        result = []
        for contentDict in cursor:
            result.append(contentDict)
        return result

    def find_all_cursor(self, db, collection, condition = None):
        self.__db = self.__Client[db]
        self.__collection = self.__db[collection]
        cursor = self.__collection.find(condition)
        return cursor

    def insert_one(self, db, collection, jsonData):
        _db = self.__Client[db]
        _collection = _db[collection]
        result = _collection.insert_one(jsonData)

    def insert_many(self, db, collection, jsonData):
        _db = self.__Client[db]
        _collection = _db[collection]
        result = _collection.insert_many(jsonData)
        return result

    def drop(self,db,collection):
        _db = self.__Client[db]
        _collection = _db[collection]
        _collection.drop()

    def close(self):
        return self.__Client.close()

json_data=j
df = DataFrame(finallyResult,index=[1])
data = df.to_dict('records')
mongoSession = MongoDbHandler('mongodb://admin:admin@119.23.222.7:27017')#A为mongodb的name,B为用户名，C为密码
mongoSession.insert_many("zhihu", "test", data)
time.sleep(2)
mongoSession.close()
```
此外，连接mongodb读取数据供kafka生产者使用，代码如下：
```python
import pandas as pd
import matplotlib.pyplot as plt
import pymongo
# 连接mongodb数据库
client = pymongo.MongoClient('mongodb://admin:admin@119.23.222.7:27017')
# 连接数据库
db = client["zhihu"]
# 数据表
dataB = db["test"]
data = pd.DataFrame(list(dataB.find()))
data.head()
data.to_csv('szHousePrice.csv',encoding='utf-8')
```
### 3.3.2 可视化软件Tableau连接MongoDB(下面以情感计数为例)
#### 使用插件mongodrdl，mongosqld将远端MongoDB映射到本地端口1037
![image.png](https://cdn.nlark.com/yuque/0/2020/png/1096029/1606653040511-434a34f9-d036-495b-8b30-90661f9c5f9f.png#align=left&display=inline&height=435&margin=%5Bobject%20Object%5D&name=image.png&originHeight=870&originWidth=1322&size=652622&status=done&style=none&width=661)
#### 使用mongoDB连接本地端口1037
![image.png](https://cdn.nlark.com/yuque/0/2020/png/1096029/1606653317218-c72d7aff-63ad-4ed2-acdb-4376e7c86fe2.png#align=left&display=inline&height=1209&margin=%5Bobject%20Object%5D&name=image.png&originHeight=2418&originWidth=3808&size=1094263&status=done&style=none&width=1904)
#### 使用可视化软件绘图
![image.png](https://cdn.nlark.com/yuque/0/2020/png/1096029/1606653843642-8a6e9959-30ef-4c0f-8b2c-9c9e47b1d8f4.png#align=left&display=inline&height=974&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1948&originWidth=2912&size=790636&status=done&style=none&width=1456)
#### 为了更好的搭配展示，从腾讯新闻的公开api中获得疫情数据，绘制疫情地图
![image.png](https://cdn.nlark.com/yuque/0/2020/png/1096029/1606654543608-f326b804-efd6-4d28-a540-48d49d33e2d8.png#align=left&display=inline&height=974&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1948&originWidth=2912&size=895914&status=done&style=none&width=1456)
最后利用pr将每个图表的视频剪辑在一起
# 4 探究结果
## 4.1 疫情初期
新冠病毒感染者最早发现于武汉，后迅速蔓延至全国乃至全球各地，研究表示，新冠病毒源自自然界的野生动物--蝙蝠，并且最早是因为人们食用野味而使新冠病毒从蝙蝠体内过渡到人体，主要通过飞沫和接触传播，早期主要表现为发热，并伴随鼻塞、流涕、打喷嚏、咽喉疼痛等症状。专家表示，戴口罩，勤洗手，定时测量体温，对疑似感染者及时进行隔离观察，对确诊感染者进行隔离治疗，是控制疫情的重要手段。
在疫情初期，知乎对于疫情的讨论相当热烈，许多关键词都被多频提及，人们的讨论还集中在武汉，口罩，野味以及病毒上。
![image.png](https://cdn.nlark.com/yuque/0/2020/png/622955/1606659711297-d6e64063-a150-4489-babb-cb74c145407b.png#align=left&display=inline&height=302&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1186&originWidth=2586&size=517354&status=done&style=none&width=659)
## 4.2 疫情高峰
疫情在全国各地全面爆发后，每天都有新增病例和新增死亡人数，人们进入了高度紧张的状态，2020年1月31日晚，新华视点和人民日报官方微博接连报道称“中国科学院上海药物所和武汉病毒所联合研究初步发现，中成药双黄连口服液可抑制新型冠状病毒”，直接引发了各地人民对线上线下各大药店双黄连存货的疯狂抢购，几乎所有品牌、店铺的双黄连口服液均显示售罄下架。
在疫情高峰期，新增病例和死亡人数迅速增多，医生，中国，隔离，医院等被广泛讨论。
        ![image.png](https://cdn.nlark.com/yuque/0/2020/png/622955/1606662052431-ffa18f55-d04b-4afc-8f60-26332750936f.png#align=left&display=inline&height=300&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1148&originWidth=2565&size=442155&status=done&style=none&width=671)


## 4.3 美国疫情
在中国的疫情逐步得到控制的同时，世界上的其他国家却开始大规模爆发新冠，而其中情形最为严重的国家非美国莫属。在美国，还有许多不带口罩出行、举行大型聚会的人，甚至还有人举行“新冠派对”，给予第一个因为参加派对而确诊新冠的人大笔奖金。美国每日新增的新冠感染人数已经达到了一个十分惊人的数值，这对全世界的新冠疫情防控影响巨大，引起了人们的持续高度关注。
在美国疫情开始流行后，可以看到中国和美国两个词语被提及最多，可以知道人们的讨论大多集中在中国与美国的比较之中。
![image.png](https://cdn.nlark.com/yuque/0/2020/png/622955/1606663100649-aed8e627-9e06-439e-b02a-592f1f870f55.png#align=left&display=inline&height=317&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1191&originWidth=2577&size=348660&status=done&style=none&width=686)
## 4.4 总结
总体可以看到，在某些特定的时间点，知乎上面新增的回答也有所不同，在不同的疫情阶段，出现的词语的频率也不相同。
最后的结果产出和实际情况大致相同。
# 5 遇到的困难和解决方案
## 5.1 配置过程中遇到的困难：

- 因为开始决定使用Python进行数据的处理，所以决定使用pyspark来进行计算，而spark3.0.1中没有pyspark的完整逻辑（缺少很多重要接口），所以改用spark2.4.7.而要想使用kafka文件夹中的接口，需要下载对应的spark-kafka-assembly jar包，开始在网上搜索很久，最终花费很多时间才找到解决办法。
- 因为服务器带宽以及存储大小限制，kafka中有很多配置文件需要改动，将jvm虚拟机需要的大小改为合适花了很多时间，因为太小会引发溢出而太大又会导致内存不足。
## 5.2 编写流计算框架过程中遇到的困难：

- 因为pyspark使用人数较少，使用其中sparkstreaming的人更加稀少，所以资料有限，所以只能自己摸索，最终找到方法，中间又产生了很多语法报错，在stackoverflow上找了很久才找到解决办法。
- 结果保存较为困难，最终找到接口save，顺利解决。
## 5.3 情感分类中遇到的困难：

- BERT原中文预训练模型过小，而且准确率和loss太差。搜索其他人实现的分类任务，借助他们标定好的语料进行fine tune，优化自己的模型，针对性训练。
- BERT官方使用tpu进行训练，而我们笔记本的CPU训练耗时过长。修改相关代码，并借助学校计算中心的GPU资源进行训练。
## 5.4 爬虫中遇到的困难及解决方案：
  由于知乎web页面不会一次性展示所有答案，需要不断下拉才会显示新的内容。
  第一次爬虫使用了chromedriver驱动chrome模拟下拉列表，最后读取网页源码再进行分析。但是这样会存在一些问题：

- 需要依赖chromedriver.exe

- 爬取速度较慢，因为是真实模拟人浏览chrome

- 爬取内容有限，不能得到所有的答案

- 当想要获得新数据时，会重复爬取已经爬到的代码


考虑到以上问题，切换爬取方法为scrapy+ajax爬取，完美解决了以上问题。

- 能够高效，快速爬取问题内容。

- 不会被封ip

- 同时没有了平台的限制，只要安装相关的依赖即可！

- 能够使用命令行定时执行爬取任务，得到新数据。



